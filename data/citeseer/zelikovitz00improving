Improving Short-Text Classification Using Unlabeled Background Knowledge to Assess Document Similarity We describe a method for improving the classification  of short text strings using a combination  of labeled training data plus a secondary corpus  of unlabeled but related longer documents. We  show that such unlabeled background knowledge  can greatly decrease error rates, particularly if  the number of examples or the size of the strings  in the training set is small. This is particularly  useful when labeling text is a labor-intensive job  and when there is a large amount of information  available about a particular problem on the World  Wide Web. Our approach views the task as one  of information integration using WHIRL, a tool  that combines database functionalities with techniques  from the information-retrieval literature.  1. Introduction  The task of classifying textual data that has been culled from sites on the World Wide Web is both difficult and intensively studied (Cohen & Hirsh, 1998; Joachims, 1998; Nigam et al., 1999). Applications of various machine learning techniqu...
