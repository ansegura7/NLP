A Comparative Evaluation of Meta-Learning Strategies over Large and Distributed Data Sets There has been considerable interest recently in various approaches to scaling up machine learning systems to large and distributed data sets. We have been studying approaches based upon the parallel application of multiple learning programs at distributed sites, followed by a meta-learning stage to combine the multiple models in a principled fashion. In this paper, we empirically determine the "best" data partitioning scheme for a selected data set to compose "appropriatelysized " subsets and we evaluate and compare three di#erent strategies, Voting, Stacking and Stacking with Correspondence Analysis (SCANN) for combining classification models trained over these subsets. We seek to find ways to e#ciently scale up to large data sets while maintaining or improving predictive performance measured by the error rate, a cost model, and the TP-FP spread.  Keywords: classification, multiple models, meta-learning, stacking, voting, correspondence analysis, data partitioning Email address of co...
