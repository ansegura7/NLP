Selective Sampling With Redundant Views Selective sampling, a form of active learning, reduces  the cost of labeling training data by asking  only for the labels of the most informative unlabeled  examples. We introduce a novel approach  to selective sampling which we call co-testing. Cotesting  can be applied to problems with redundant  views (i.e., problems with multiple disjoint sets of  attributes that can be used for learning). We analyze  the most general algorithm in the co-testing  family, naive co-testing, which can be used with  virtually any type of learner. Naive co-testing  simply selects at random an example on which the  existing views disagree. We applied our algorithm  to a variety of domains, including three real-world  problems: wrapper induction, Web page classification,  and discourse trees parsing. The empirical  results show that besides reducing the number of  labeled examples, naive co-testing may also boost  the classification accuracy.  Introduction  In order to learn a classifier, supervised learn...
