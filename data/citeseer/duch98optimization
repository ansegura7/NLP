Optimization and Global Minimization Methods Suitable for Neural Networks Neural networks are usually trained using local, gradient-based procedures. Such methods frequently find suboptimal solutions being trapped in local minima. Optimization of neural structures and global minimization methods applied to network cost functions have strong influence on all aspects of network performance. Recently genetic algorithms are frequently combined with neural methods to select best architectures and avoid drawbacks of local minimization methods. Many other global minimization methods are suitable for that purpose, although they are used rather rarely in this context. This paper provides a survey of such global methods, including some aspects of genetic algorithms. CONTENTS 1 Introduction 2 2 Monte Carlo and its improvements 4 3 Simulated annealing and its variants 6 3.1 Adaptive Simulated Annealing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.2 Alopex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ....
