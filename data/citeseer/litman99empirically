Empirically Evaluating an Adaptable Spoken Dialogue System Recent technological advances have made it possible to build real-time, interactive  spoken dialogue systems for a wide variety of applications. However, when users  do not respect the limitations of such systems, performance typically degrades. Although  users differ with respect to their knowledge of system limitations, and although different  dialogue strategies make system limitations more apparent to users, most current systems  do not try to improve performance by adapting dialogue behavior to individual users. This  paper presents an empirical evaluation of TOOT, an adaptable spoken dialogue system for  retrieving train schedules on the web. We conduct an experiment in which 20 users carry  out 4 tasks with both adaptable and non-adaptable versions of TOOT, resulting in a corpus  of 80 dialogues. The values for a wide range of evaluation measures are then extracted from  this corpus. Our results show that adaptable TOOT generally outperforms non-adaptable  TOOT, and that the utility of adaptation depends on TOOT's initial dialogue strategies.
