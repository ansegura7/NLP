Algorithm-Directed Exploration for Model-Based Reinforcement Learning in Factored MDPs One of the central challenges in reinforcement  learning is to balance the exploration/exploitation  tradeoff while scaling up to large problems. Although  model-based reinforcement learning has  been less prominent than value-based methods in  addressing these challenges, recent progress has  generated renewed interest in pursuing modelbased  approaches: Theoretical work on the exploration  /exploitation tradeoff has yielded provably  sound model-based algorithms such as E    Rmax , while work on factored MDP representations  has yielded model-based algorithms that can  scale up to large problems. Recently the benefits  of both achievements have been combined in the    algorithm of Kearns and Koller. In  this paper, we address a significant shortcoming  of Factored E    : namely that it requires an oracle  planner that cannot be feasibly implemented. We  propose an alternative approach that uses a practical  approximate planner, approximate linear programming,  that maintains desirable properties. Further,  we develop an exploration strategy that is targeted  toward improving the performance of the linear  programming algorithm, rather than an oracle  planner. This leads to a simple exploration strategy  that visits states relevant to tightening the LP solution,  and achieves sample efficiency logarithmic in  the size of the problem description. Our experimental  results show that the targeted approach performs  better than using approximate planning for implementing  either Factored E    or Factored Rmax .
