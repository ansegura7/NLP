Lifelike Gesture Synthesis and Timing for Conversational Agents Besides the inclusion of gesture recognition devices as an intuitive input modality, the synthesis of lifelike gesture is finding growing attention in human-computer interface research. In particular, the generation of synthetic gesture in connection with text-to-speech systems is one of the goals for embodied conversational agents which have become a new paradigm for the study of gesture and for human-computer interface [1]. Embodied conversational agents are computer-generated characters that demonstrate similar properties as humans in face-to-face conversation, including the ability to produce and respond to verbal and nonverbal communication. They may represent the computer in an interaction with a human or represent their human users as &quot;avatars &quot; in a computational environment. In this context, this contribution focusses on an approach for synthesizing lifelike gestures for an articulated virtual agent, with particular emphasis on how to achieve temporal coordination with external information such as the signal generated by a text-to-speech system. The context of this research is the conception of an &quot;articulated communicator &quot; that conducts multimodal dialogue with a human partner in cooperating on a construction task. Gesture production and performance in humans is a complex and multi-stage process.
