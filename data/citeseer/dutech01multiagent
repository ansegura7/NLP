Multi-Agent Systems by Incremental Gradient Reinforcement Learning situated with local and scalable  perceptions,  .  have identical capabilities,  .  are possibly heterogeneous,  .  cooperate,  .  do not directly communicate.  Each agent learns its behavior on its own.  11/22  IJCAI'01  # # # # # # Bloc merging (the problem)  .  reward:  +3 if blocs are merged  .  actions:  N  W E  S  .  perceptions:  -- dir(agent)  #4  -- dir(yellow bloc)    -- dir(blue bloc)    -- near(yellow bloc)    -- near(blue bloc)    total    1024/4  (MDP with 2 agents and 2 cubes for an 8    8 world: 15.248.024 states !!!)  12/22  IJCAI'01  # # # # # # An agent learns a policy:  # :  But the <  O, A  > is not markovian:  .  convergence is not assured,  .  stochastic policies should perform better  [SJJ94].  1b  1a  A (-R)  B(+R)  B(-R)  A(+R)  13/22  IJCAI'01  # # # # # # Multi-agent framework:  .  each agent considers other  agents as part of the environment,    .  all agents learn, therefore  evolve,    unpredictable transitions.  Q-learning    Baxter's gradient descent  A
