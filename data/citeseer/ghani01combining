Combining Labeled and Unlabeled Data for Text Classification With a Large Number of Categories A major concern with supervised learning techniques for text classification  is that they often require a large number of labeled examples to learn  accurately. One way to reduce the amount of labeled data required is to  develop algorithms that can learn effectively from a small number of labeled  examples augmented with a large number of unlabeled examples.  In this paper, we develop a framework to incorporate unlabeled data in  the Error-Correcting Output Coding (ECOC) setup by decomposing multiclass  problems into multiple binary problems and then use Co-Training  to learn the individual binary classification problems. We show that our  method is especially useful for classification tasks involving a large number  of categories where Co-training doesn't perform very well by itself  and when combined with ECOC, outperforms several other algorithms  that combine labeled and unlabeled data for text classification.  1 
