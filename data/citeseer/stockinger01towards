Towards a Cost Model for Distributed and Replicated Data Stores Large, Petabyte-scale data stores need detailed design considerations about distributing and replicating particular parts of the data store in a cost-effective way. Technical issues need to be analysed and, based on these constraints, an optimisation problem can be formulated. In this paper we provide a novel cost model for building a world-wide distributed Petabyte data store which will be in place starting from 2005 at CERN and its collaborating, world-wide distributed institutes. We will elaborate on a framework for assessing potential system costs and influences which are essential for the design of the data store.  1 Introduction  With the growth of the Internet in the last couple of years and expanding technologies in database research, data warehousing, networking and data storage, large distributed data stores with data amounts in the range of Petabytes are emerging [16]. Not only the choice of the optimal data storage system (relational or object-oriented databases, flat files...
