Actor-Critic Algorithms We propose and analyze a class of actor-critic algorithms for  simulation-based optimization of a Markov decision process over  a parameterized family of randomized stationary policies. These  are two-time-scale algorithms in which the critic uses TD learning  with a linear approximation architecture and the actor is updated  in an approximate gradient direction based on information provided  by the critic. We show that the features for the critic should  span a subspace prescribed by the choice of parameterization of the  actor. We conclude by discussing convergence properties and some  open problems.  1 Introduction  The vast majority of Reinforcement Learning (RL) [9] and Neuro-Dynamic Programming (NDP) [1] methods fall into one of the following two categories:  (a) Actor-only methods work with a parameterized family of policies. The gradient of the performance, with respect to the actor parameters, is directly estimated by simulation, and the parameters are updated in a direction o...
