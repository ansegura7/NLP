Automatic Facial Expression Interpretation: Where Human-Computer Interaction, Artificial Intelligence and Cognitive Science Intersect this paper is to attempt to bring together people, results and questions from these three different disciplines -- HCI, AI, and Cognitive Science -- to explore the potential of building computer interfaces which understand and respond to the richness of the information conveyed in the human face. Until recently, information has been conveyed from the computer to the user mainly via the visual channel, whereas inputs from the user to the computer have been made from the keyboard and pointing devices via the user's motor channel. The recent emergence of multimodal interfaces as our everyday tools might restore a better balance between our physiology and sensory/motor skills, and impact (for the better we hope), the richness of activities we will find ourselves involved in. Given recent progress in user-interface primitives composed of gesture, speech, context and affect, it seems feasible to design environments which do not impose themselves as computer environments, but have a much more natural feeling associated with them.
