{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Created by Andrés Segura Tinoco**\n",
    "- **Created on June 09, 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim** is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Python libraries\n",
    "import io\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLP libraries from gensim and spacy\n",
    "from gensim.models import Word2Vec\n",
    "import spacy.lang.en as en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Read natural text from a book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to read a plain text file\n",
    "def read_text_file(file_path):\n",
    "    text = \"\"\n",
    "    with io.open(file_path, 'r', encoding = 'ISO-8859-1') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    return text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576467"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get text sample\n",
    "file_path = \"../data/en/The Adventures of Sherlock Holmes - Arthur Conan Doyle.txt\"\n",
    "plain_text = read_text_file(file_path)\n",
    "len(plain_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nProject Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\\n\\nThis eBook is for the use of anyone anywhere at no cost and with\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\nre-use it under the terms of the Project Gutenberg License included\\nwith this eBook or online at www.gutenberg.net\\n\\n\\nTitle: The Adventures of Sherlock Holmes\\n\\nAuthor: Arthur Conan Doyle\\n\\nRelease Date: November 29, 2002 [EBook #1661]\\nLast Updated: May 20, 2019\\n\\nLanguage: English\\n\\nCharacter set encoding: UTF-8\\n\\n*** START OF THIS PROJECT GUTENBERG EBOOK THE ADVENTURES OF SHERLOCK HOLMES ***\\n\\n\\n\\nProduced by an anonymous Project Gutenberg volunteer and Jose Menendez\\n\\n\\n\\ncover\\n\\n\\n\\nThe Adventures of Sherlock Holmes\\n\\n\\n\\nby Arthur Conan Doyle\\n\\n\\n\\nContents\\n\\n\\n   I.     A Scandal in Bohemia\\n   II.    The Red-Headed League\\n   III.   A Case of Identity\\n   IV.    The Boscombe Valley Mystery\\n   V.     The Five Orange Pips\\n   VI.    The Man with the Twisted Lip\\n   VII.   The Adventure of the Blue C\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 1000 characters of document\n",
    "plain_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Tokenize and remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.project gutenberg s the adventures of sherlock holmes by arthur conan doyle.this ebook is for the use of anyone anywhere at no cost and with.almost no restrictions whatsoever. you may copy it give it away or.re use it under the terms of the project gutenberg license included.with this ebook or online at www.gutenberg.net.title the adventures of sherlock holmes.author arthur conan doyle.release date november ebook .last updated may .language english.character set encoding utf . start of this project gutenberg ebook the adventures of sherlock holmes .produced by an anonymous project gutenberg volunteer and jose menendez.cover.the adventures of sherlock holmes.by arthur conan doyle.contents. i. a scandal in bohemia. ii. the red headed league. iii. a case of identity. iv. the boscombe valley mystery. v. the five orange pips. vi. the man with the twisted lip. vii. the adventure of the blue carbuncle. viii. the adventure of the speckled band. ix. the adventure of the engineer s thumb. x. th'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaing the text\n",
    "clean_text = plain_text.lower()\n",
    "clean_text = clean_text.replace('\\n', '.')\n",
    "clean_text = re.sub('[^a-zA-Z.]', ' ', clean_text)\n",
    "clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "clean_text = re.sub(r'\\.+', \".\", clean_text)\n",
    "clean_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14592"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize text in sentences\n",
    "sentence_list = clean_text.split('.')\n",
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['project',\n",
       "  'gutenberg',\n",
       "  's',\n",
       "  'the',\n",
       "  'adventures',\n",
       "  'of',\n",
       "  'sherlock',\n",
       "  'holmes',\n",
       "  'by',\n",
       "  'arthur',\n",
       "  'conan',\n",
       "  'doyle'],\n",
       " ['this',\n",
       "  'ebook',\n",
       "  'is',\n",
       "  'for',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'anyone',\n",
       "  'anywhere',\n",
       "  'at',\n",
       "  'no',\n",
       "  'cost',\n",
       "  'and',\n",
       "  'with'],\n",
       " ['almost', 'no', 'restrictions', 'whatsoever'],\n",
       " ['you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or'],\n",
       " ['re',\n",
       "  'use',\n",
       "  'it',\n",
       "  'under',\n",
       "  'the',\n",
       "  'terms',\n",
       "  'of',\n",
       "  'the',\n",
       "  'project',\n",
       "  'gutenberg',\n",
       "  'license',\n",
       "  'included'],\n",
       " ['with', 'this', 'ebook', 'or', 'online', 'at', 'www'],\n",
       " ['gutenberg'],\n",
       " ['net'],\n",
       " ['title', 'the', 'adventures', 'of', 'sherlock', 'holmes'],\n",
       " ['author', 'arthur', 'conan', 'doyle']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize sentences in words\n",
    "word_list = [sentence.split() for sentence in sentence_list if len(sentence.split()) > 0]\n",
    "word_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not', 'empty', 'afterwards', 'that', 'each', 'anyhow', 'she', 'themselves', 'hereby', 'their', 'same', 'who', 'now', 'meanwhile', 'as', 'and', 'since', 'with', 'without', 'was', 'please', 'to', 'herein', 'otherwise', 'under', 'however', 'can', 'herself', 'make', 'never', 'such', 'then', 'eight', 'our', 'these', 'amount', 'sixty', 'nor', 'just', 'forty', 'often', 'could', 'less', 'few', 'every', 'no', 'most', 'here', 'me', 'were', 'fifty', 'whereafter', 'anywhere', \"'re\", 'but', 'has', 'move', 'should', 'wherein', 'some', 'on', 'yours', 'first', 'if', 'next', 'ours', 'serious', 'thereupon', 'used', 'wherever', 'amongst', 'in', 'latter', 'somehow', 'side', 'almost', 'do', 'upon', 'your', 'through', 'been', '‘s', 'except', 'full', 'what', 'into', 'show', 'ten', 'call', 'several', 'there', 'yourself', 'third', \"'d\", 'beforehand', 'least', 'seems', 'of', 'how', 'so', '‘m', 'alone', 'beyond', 'thus', 'nine', 'mine', 'across', 'while', 'are', 'due', 'must', 'will', 'together', 'both', 'within', 'against', 'eleven', 'would', 'therefore', 'became', 'out', 'myself', 'else', 'we', '’d', 'too', 'where', 'whereupon', 'own', 'by', 'itself', 'everywhere', 'thence', 'nowhere', 'either', '‘ll', \"'ll\", 'at', 'made', 'ourselves', 'whence', 'everyone', 'latterly', 'any', 'regarding', 'see', 'via', 'because', 'cannot', 'becomes', 'done', 'go', 'someone', 'becoming', 'doing', 'anything', 'seeming', \"'s\", 'five', 'moreover', '’re', 'up', 'whom', \"'m\", 'ever', 'besides', 'always', 'n’t', '’ve', 'or', 'front', '‘ve', 'top', 'whereas', 'they', 'seemed', 're', 'give', 'before', 'thru', 'why', 'those', 'well', 'whoever', 'between', 'from', 'more', 'part', 'them', 'being', 'ca', 'his', 'back', 'quite', 'further', 'noone', 'again', 'other', 'sometimes', 'beside', 'whose', 'per', 'did', 'than', 'when', 'thereby', 'hereupon', 'namely', 'whether', 'although', 'you', 'below', 'though', 'even', 'formerly', 'name', 'throughout', \"n't\", 'onto', 'also', 'rather', 'whatever', 'whereby', 'might', 'therein', 'its', 'become', 'using', 'along', 'indeed', 'only', 'an', 'us', 'may', 'none', 'various', 'fifteen', 'four', 'yourselves', 'twenty', 'hundred', 'unless', 'does', 'he', 'hereafter', 'perhaps', 'down', 'six', 'whither', '’s', 'during', 'anyone', 'seem', '‘re', 'which', 'another', 'mostly', 'all', 'i', 'many', '’ll', 'had', 'nobody', 'one', 'yet', 'everything', 'off', 'for', 'over', 'former', 'put', 'sometime', 'it', 'others', 'something', 'two', 'towards', '’m', 'behind', 'bottom', 'until', 'nevertheless', '‘d', 'above', 'after', 'my', 'among', 'take', 'whenever', 'really', 'thereafter', 'last', 'already', 'elsewhere', 'get', 'very', 'her', 'be', 'enough', 'around', 'hers', 'somewhere', 'anyway', 'this', 'much', 'n‘t', 'about', 'three', 'twelve', \"'ve\", 'say', 'keep', 'neither', 'still', 'hence', 'is', 'nothing', 'the', 'him', 'toward', 'have', 'a', 'himself', 'whole', 'once', 'am'}\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = en.stop_words.STOP_WORDS\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['project',\n",
       "  'gutenberg',\n",
       "  'adventures',\n",
       "  'sherlock',\n",
       "  'holmes',\n",
       "  'arthur',\n",
       "  'conan',\n",
       "  'doyle'],\n",
       " ['ebook', 'use', 'cost'],\n",
       " ['restrictions', 'whatsoever'],\n",
       " ['copy', 'away'],\n",
       " ['use', 'terms', 'project', 'gutenberg', 'license', 'included'],\n",
       " ['ebook', 'online', 'www'],\n",
       " ['gutenberg'],\n",
       " ['net'],\n",
       " ['title', 'adventures', 'sherlock', 'holmes'],\n",
       " ['author', 'arthur', 'conan', 'doyle']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "all_words = []\n",
    "for ix in range(len(word_list)):\n",
    "    all_words.append([word for word in word_list[ix] if (word not in stopwords_en and len(word) > 2)])\n",
    "\n",
    "all_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Create a Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Word2Vec model\n",
    "word2vec = Word2Vec(all_words, min_count = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4072"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show vocabulary size\n",
    "vocabulary = word2vec.wv.vocab  \n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00571524,  0.0037815 , -0.0029851 , -0.00234666, -0.00384644,\n",
       "        0.0026138 ,  0.00376731, -0.00350076, -0.00345531, -0.00123497,\n",
       "       -0.00392786, -0.00487326,  0.002396  ,  0.00065813, -0.00089581,\n",
       "        0.00288083,  0.003073  , -0.00023033,  0.00064188,  0.00469708,\n",
       "        0.00057567, -0.00287395,  0.00170696, -0.00220402, -0.00369562,\n",
       "        0.00048359, -0.00554605,  0.00117496,  0.00097162,  0.00056364,\n",
       "       -0.00300056,  0.00420297,  0.00553688,  0.00070874, -0.0008803 ,\n",
       "       -0.00219831,  0.00295734,  0.00202147, -0.00204766,  0.00069915,\n",
       "        0.00155343, -0.00173383,  0.00101644,  0.0037438 , -0.00295153,\n",
       "       -0.0027267 , -0.00293964, -0.00352298, -0.00384365,  0.00290935,\n",
       "       -0.00594365, -0.00033689, -0.00500323, -0.0002361 , -0.00169291,\n",
       "        0.00290482, -0.0013868 ,  0.00172921,  0.00102748, -0.00260645,\n",
       "        0.00270165, -0.002158  ,  0.00060265,  0.00046697, -0.00544463,\n",
       "       -0.00219709, -0.00153067, -0.00328287, -0.00191149, -0.00256149,\n",
       "        0.00487508, -0.00325485,  0.00222084, -0.00238673,  0.0008818 ,\n",
       "       -0.00223221,  0.00135853,  0.00236858, -0.00180198, -0.0013227 ,\n",
       "        0.00433142, -0.0015103 ,  0.00303107, -0.00276974,  0.00098357,\n",
       "        0.00057665,  0.00166037, -0.00394594, -0.00010403,  0.00187582,\n",
       "       -0.00525217, -0.00191993,  0.00092555,  0.00087304,  0.00067075,\n",
       "       -0.00120125,  0.00246809, -0.00013627, -0.00433768,  0.00246021],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show 'project' vector\n",
    "word2vec.wv['project']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
